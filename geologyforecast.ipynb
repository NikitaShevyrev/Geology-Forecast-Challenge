{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc341c01",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-01T15:23:15.017826Z",
     "iopub.status.busy": "2025-06-01T15:23:15.017543Z",
     "iopub.status.idle": "2025-06-01T15:23:15.958649Z",
     "shell.execute_reply": "2025-06-01T15:23:15.957937Z"
    },
    "papermill": {
     "duration": 0.950328,
     "end_time": "2025-06-01T15:23:15.960198",
     "exception": false,
     "start_time": "2025-06-01T15:23:15.009870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    " \n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        pass\n",
    "        # print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b6c4fd",
   "metadata": {
    "papermill": {
     "duration": 0.005476,
     "end_time": "2025-06-01T15:23:15.972106",
     "exception": false,
     "start_time": "2025-06-01T15:23:15.966630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Geology Forecast Challenge ‚Äî Final Submission\n",
    "\n",
    "This notebook contains the **full pipeline** for my solution:\n",
    "- ‚úÖ Synthetic data generation (optional)\n",
    "- ‚úÖ Data loading and preparation\n",
    "- ‚úÖ Model definition (LSTM + Attention + Attention projection)\n",
    "- ‚úÖ Training (commented out)\n",
    "- ‚úÖ Inference using pretrained weights from a private Kaggle dataset\n",
    "\n",
    "üì¶ **GitHub repository** (temporarily private for final review):\n",
    "- https://github.com/NikitaShevyrev/Geology-Forecast-Challenge#\n",
    "- üîê This repository will be made public as required by the competition rules within 5 days of the competition‚Äôs end.\n",
    "- üì¨ Please contact me via Kaggle if you require early access for evaluation.\n",
    "\n",
    "üìÑ `submission.csv` is generated in the final cell using provided `test.csv` and trained fold weights (attached via private Kaggle dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae5060f",
   "metadata": {
    "papermill": {
     "duration": 0.005364,
     "end_time": "2025-06-01T15:23:15.983121",
     "exception": false,
     "start_time": "2025-06-01T15:23:15.977757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Additional data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4bb673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:23:15.995397Z",
     "iopub.status.busy": "2025-06-01T15:23:15.994906Z",
     "iopub.status.idle": "2025-06-01T15:24:03.918987Z",
     "shell.execute_reply": "2025-06-01T15:24:03.918001Z"
    },
    "papermill": {
     "duration": 47.94025,
     "end_time": "2025-06-01T15:24:03.928973",
     "exception": false,
     "start_time": "2025-06-01T15:23:15.988723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:11<00:00, 10.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 38253 synthetic samples to /kaggle/working/synthetic_train.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "import glob\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(2025)\n",
    "\n",
    "OUTPUT_FILE = \"/kaggle/working/synthetic_train.csv\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "def generate_realizations(base, num=10, noise_level=0.5):\n",
    "    base = base.copy()\n",
    "    realizations = [base]\n",
    "    for _ in range(1, num):\n",
    "        noise = np.random.randn(len(base)) * noise_level\n",
    "        perturb = np.convolve(noise, np.ones(20)/20, mode='same')  # Smooth noise\n",
    "        perturbed = base + perturb\n",
    "        realizations.append(perturbed)\n",
    "    return np.array(realizations)\n",
    "\n",
    "def process_file(path, window_size=600, min_valid=610, max_chunks=20):\n",
    "    df = pd.read_csv(path)\n",
    "    x_raw = df['VS_APPROX_adjusted'].values\n",
    "    z_raw = df['HORIZON_Z_adjusted'].values\n",
    "\n",
    "    # Interpolate on 1-foot grid\n",
    "    x_new = np.arange(0, x_raw.max(), 1.0)\n",
    "    f_interp = interp1d(x_raw, z_raw, kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n",
    "    z_new = f_interp(x_new)\n",
    "\n",
    "    rows = []\n",
    "    if len(z_new) < min_valid:\n",
    "        return rows\n",
    "\n",
    "    num_chunks = 0\n",
    "    attempts = 0\n",
    "    while num_chunks < max_chunks: #  and attempts < 50\n",
    "        start = np.random.randint(0, len(z_new) - window_size)\n",
    "        chunk = z_new[start:start + window_size].copy()\n",
    "        chunk -= chunk[299]  # normalize so Z(0)=0\n",
    "\n",
    "        # Simulate drilling by hiding part of left context\n",
    "        if np.random.rand() > 0.6:\n",
    "            hide_up_to = np.random.randint(0, 250)\n",
    "        else:\n",
    "            hide_up_to = 0\n",
    "        chunk_with_nans = chunk.copy()\n",
    "        chunk_with_nans[:hide_up_to] = np.nan\n",
    "\n",
    "        realizations = generate_realizations(chunk[300:], num=1) # 10\n",
    "        output = {\n",
    "            'geology_id': str(uuid.uuid4()),\n",
    "        }\n",
    "        \n",
    "        for i in range(300):\n",
    "            output[str(i - 299)] = chunk_with_nans[i]\n",
    "        for i in range(300):\n",
    "            output[str(i + 1)] = realizations[0, i]\n",
    "        \n",
    "        # for r in range(1, 10):\n",
    "        #     for i in range(300):\n",
    "        #         output[f\"r_{r}_pos_{i+1}\"] = realizations[r, i]\n",
    "\n",
    "        rows.append(output)\n",
    "        num_chunks += 1\n",
    "        attempts += 1\n",
    "\n",
    "    return rows\n",
    "\n",
    "def main():\n",
    "    all_files = glob.glob(\"/kaggle/input/geology-forecast-challenge-open/data/train_raw/*.csv\")\n",
    "    all_data = []\n",
    "    for f in tqdm(all_files):\n",
    "        all_data.extend(process_file(f, max_chunks=38190//len(all_files)+1))\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Saved {len(df)} synthetic samples to {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcbb2d3",
   "metadata": {
    "papermill": {
     "duration": 0.008369,
     "end_time": "2025-06-01T15:24:03.946198",
     "exception": false,
     "start_time": "2025-06-01T15:24:03.937829",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Additional imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd10749a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:03.964223Z",
     "iopub.status.busy": "2025-06-01T15:24:03.963947Z",
     "iopub.status.idle": "2025-06-01T15:24:07.435659Z",
     "shell.execute_reply": "2025-06-01T15:24:07.434967Z"
    },
    "papermill": {
     "duration": 3.482311,
     "end_time": "2025-06-01T15:24:07.437180",
     "exception": false,
     "start_time": "2025-06-01T15:24:03.954869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf6874d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:07.455807Z",
     "iopub.status.busy": "2025-06-01T15:24:07.455457Z",
     "iopub.status.idle": "2025-06-01T15:24:09.484331Z",
     "shell.execute_reply": "2025-06-01T15:24:09.483415Z"
    },
    "papermill": {
     "duration": 2.039499,
     "end_time": "2025-06-01T15:24:09.485831",
     "exception": false,
     "start_time": "2025-06-01T15:24:07.446332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "import wandb\n",
    "from google.colab import userdata\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d622e",
   "metadata": {
    "papermill": {
     "duration": 0.008237,
     "end_time": "2025-06-01T15:24:09.502980",
     "exception": false,
     "start_time": "2025-06-01T15:24:09.494743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fixing seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ffb539",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:09.524826Z",
     "iopub.status.busy": "2025-06-01T15:24:09.524521Z",
     "iopub.status.idle": "2025-06-01T15:24:09.532990Z",
     "shell.execute_reply": "2025-06-01T15:24:09.532131Z"
    },
    "papermill": {
     "duration": 0.023054,
     "end_time": "2025-06-01T15:24:09.534399",
     "exception": false,
     "start_time": "2025-06-01T15:24:09.511345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# üîπ Reproducibility\n",
    "def seed_everything(seed=2025):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c22a27d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:09.552478Z",
     "iopub.status.busy": "2025-06-01T15:24:09.552259Z",
     "iopub.status.idle": "2025-06-01T15:24:09.604680Z",
     "shell.execute_reply": "2025-06-01T15:24:09.603817Z"
    },
    "papermill": {
     "duration": 0.063044,
     "end_time": "2025-06-01T15:24:09.606077",
     "exception": false,
     "start_time": "2025-06-01T15:24:09.543033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd841c",
   "metadata": {
    "papermill": {
     "duration": 0.008352,
     "end_time": "2025-06-01T15:24:09.623388",
     "exception": false,
     "start_time": "2025-06-01T15:24:09.615036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2709e21e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:09.641214Z",
     "iopub.status.busy": "2025-06-01T15:24:09.640932Z",
     "iopub.status.idle": "2025-06-01T15:24:14.137836Z",
     "shell.execute_reply": "2025-06-01T15:24:14.136955Z"
    },
    "papermill": {
     "duration": 4.507353,
     "end_time": "2025-06-01T15:24:14.139272",
     "exception": false,
     "start_time": "2025-06-01T15:24:09.631919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38190, 601)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_train_df = pd.read_csv(\"/kaggle/working/synthetic_train.csv\")\n",
    "synth_train_df = synth_train_df.iloc[:38190,:]\n",
    "synth_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d1dc400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:14.160543Z",
     "iopub.status.busy": "2025-06-01T15:24:14.160314Z",
     "iopub.status.idle": "2025-06-01T15:24:16.244791Z",
     "shell.execute_reply": "2025-06-01T15:24:16.243793Z"
    },
    "papermill": {
     "duration": 2.096301,
     "end_time": "2025-06-01T15:24:16.246239",
     "exception": false,
     "start_time": "2025-06-01T15:24:14.149938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detected 10 realizations.\n"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ Load Data\n",
    "train_df = pd.read_csv(\"/kaggle/input/geology-forecast-challenge-open/data/train.csv\")\n",
    "train_df = train_df.iloc[:,:601]\n",
    "train_df = pd.concat([train_df, synth_train_df], ignore_index=True)\n",
    "test_df = pd.read_csv(\"/kaggle/input/geology-forecast-challenge-open/data/test.csv\")\n",
    "submission_template = pd.read_csv('/kaggle/input/geology-forecast-challenge-open/data/sample_submission.csv')\n",
    "\n",
    "FEATURES = [col for col in test_df.columns if col != \"geology_id\"]\n",
    "REALIZATIONS = [col for col in submission_template.columns if col != \"geology_id\"]\n",
    "NUM_REALIZATIONS = len(REALIZATIONS) // 300\n",
    "\n",
    "print(f\"üîç Detected {NUM_REALIZATIONS} realizations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a9f41e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:16.264676Z",
     "iopub.status.busy": "2025-06-01T15:24:16.264444Z",
     "iopub.status.idle": "2025-06-01T15:24:36.992808Z",
     "shell.execute_reply": "2025-06-01T15:24:36.991914Z"
    },
    "papermill": {
     "duration": 20.739014,
     "end_time": "2025-06-01T15:24:36.994185",
     "exception": false,
     "start_time": "2025-06-01T15:24:16.255171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3970, 6001)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_new = ['geology_id']\n",
    "for r in range(NUM_REALIZATIONS):\n",
    "    for i in range(-299,301):\n",
    "        columns_new.append(f'r_{r}_pos_{i}')\n",
    "\n",
    "train_df_new = pd.DataFrame(\n",
    "    data=np.concatenate((np.full((train_df.shape[0]//10,1), 'none'), train_df.iloc[:,1:].values.reshape((-1,10*600))), axis=1),\n",
    "    columns=columns_new\n",
    ")\n",
    "train_df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4dcdb6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:37.012694Z",
     "iopub.status.busy": "2025-06-01T15:24:37.012440Z",
     "iopub.status.idle": "2025-06-01T15:24:37.018553Z",
     "shell.execute_reply": "2025-06-01T15:24:37.017719Z"
    },
    "papermill": {
     "duration": 0.016577,
     "end_time": "2025-06-01T15:24:37.019867",
     "exception": false,
     "start_time": "2025-06-01T15:24:37.003290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_FEATURES = []\n",
    "for r in range(NUM_REALIZATIONS):\n",
    "    for i in range(-299,1):\n",
    "        train_FEATURES.append(f'r_{r}_pos_{i}')\n",
    "\n",
    "train_REALIZATIONS = []\n",
    "for r in range(NUM_REALIZATIONS):\n",
    "    for i in range(1,301):\n",
    "        train_REALIZATIONS.append(f'r_{r}_pos_{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1001a6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:37.037936Z",
     "iopub.status.busy": "2025-06-01T15:24:37.037710Z",
     "iopub.status.idle": "2025-06-01T15:24:43.373123Z",
     "shell.execute_reply": "2025-06-01T15:24:43.372363Z"
    },
    "papermill": {
     "duration": 6.345833,
     "end_time": "2025-06-01T15:24:43.374338",
     "exception": false,
     "start_time": "2025-06-01T15:24:37.028505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3970, 6001)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df_new\n",
    "train_df = train_df.iloc[:,1:]\n",
    "train_df = train_df.astype('float64')\n",
    "train_df.insert(0, 'geology_id', 'none')\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24e27463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:43.393075Z",
     "iopub.status.busy": "2025-06-01T15:24:43.392835Z",
     "iopub.status.idle": "2025-06-01T15:24:44.277035Z",
     "shell.execute_reply": "2025-06-01T15:24:44.276378Z"
    },
    "papermill": {
     "duration": 0.895005,
     "end_time": "2025-06-01T15:24:44.278593",
     "exception": false,
     "start_time": "2025-06-01T15:24:43.383588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# üîß Replacing nans\n",
    "train_df.iloc[:, 1:] = train_df.iloc[:, 1:].fillna(0)\n",
    "test_df.iloc[:, 1:] = test_df.iloc[:, 1:].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea2d802",
   "metadata": {
    "papermill": {
     "duration": 0.008627,
     "end_time": "2025-06-01T15:24:44.296479",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.287852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset holding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d00ae0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:44.314793Z",
     "iopub.status.busy": "2025-06-01T15:24:44.314553Z",
     "iopub.status.idle": "2025-06-01T15:24:44.319044Z",
     "shell.execute_reply": "2025-06-01T15:24:44.318408Z"
    },
    "papermill": {
     "duration": 0.015026,
     "end_time": "2025-06-01T15:24:44.320229",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.305203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GeologyDataset(Dataset):\n",
    "    def __init__(self, features, targets=None, realization_ids=None, is_test=False):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.realization_ids = realization_ids\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx].reshape(-1, 1)  # [50, 1]\n",
    "\n",
    "        if self.is_test:\n",
    "            return x\n",
    "        else:\n",
    "            y = self.targets[idx]  # [300]\n",
    "            rid = self.realization_ids[idx]  # scalar\n",
    "            return x, y, rid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7fe944",
   "metadata": {
    "papermill": {
     "duration": 0.00852,
     "end_time": "2025-06-01T15:24:44.337242",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.328722",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "658b555b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:44.355352Z",
     "iopub.status.busy": "2025-06-01T15:24:44.355097Z",
     "iopub.status.idle": "2025-06-01T15:24:44.364722Z",
     "shell.execute_reply": "2025-06-01T15:24:44.364073Z"
    },
    "papermill": {
     "duration": 0.019898,
     "end_time": "2025-06-01T15:24:44.365789",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.345891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# üß† Model\n",
    "class ParallelLSTMWithAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        output_size,\n",
    "        dropout=0.2,\n",
    "        num_realizations=10,\n",
    "        realization_emb_dim=16,\n",
    "        use_multihead=True,\n",
    "        num_heads=4,\n",
    "        fusion_method='concat'  # options: 'concat', 'add', 'gated'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.realization_embedding = nn.Embedding(num_realizations, realization_emb_dim)\n",
    "        self.fusion_method = fusion_method\n",
    "        self.hidden_size = hidden_size\n",
    "        # LSTM Branch\n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            nn.LSTM(\n",
    "                input_size=input_size if i == 0 else hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=1,\n",
    "                batch_first=True,\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "        self.lstm_norms = nn.ModuleList([nn.LayerNorm(hidden_size) for _ in range(num_layers)])\n",
    "        # Attention Branch\n",
    "        self.attn_input = nn.Linear(input_size, hidden_size)\n",
    "        if use_multihead:\n",
    "            self.attn = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n",
    "        else:\n",
    "            self.attn = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=1, batch_first=True)\n",
    "        # Learnable Basis Projection Branch\n",
    "        self.num_bases = 16\n",
    "        self.basis_proj = nn.Linear(input_size, self.num_bases)\n",
    "        self.basis_decoder = nn.Linear(self.num_bases, hidden_size)\n",
    "        # Fusion\n",
    "        fusion_input_size = hidden_size * 3 if fusion_method == 'concat' else hidden_size\n",
    "        if fusion_method == 'gated':\n",
    "            self.gate_fc = nn.Linear(hidden_size * 2, hidden_size)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "        self.fc1 = nn.Linear(fusion_input_size + realization_emb_dim, hidden_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x, realization_ids):\n",
    "        # LSTM branch\n",
    "        lstm_out = x\n",
    "        for lstm, norm in zip(self.lstm_layers, self.lstm_norms):\n",
    "            residual = lstm_out\n",
    "            lstm_out, _ = lstm(lstm_out)\n",
    "            lstm_out = norm(lstm_out + residual)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Last step\n",
    "        # Attention branch\n",
    "        attn_input = self.attn_input(x)\n",
    "        attn_out, _ = self.attn(attn_input, attn_input, attn_input)\n",
    "        attn_out = attn_out.max(dim=1).values\n",
    "        # Learnable Basis Projection branch\n",
    "        basis_coeffs = self.basis_proj(x).max(dim=1).values  # [B, num_bases]\n",
    "        basis_out = self.basis_decoder(basis_coeffs)   # [B, H]\n",
    "        # Fusion\n",
    "        if self.fusion_method == 'add':\n",
    "            fused = lstm_out + attn_out + basis_out\n",
    "        elif self.fusion_method == 'gated':\n",
    "            gate_attn = self.sigmoid(self.gate_fc(torch.cat([lstm_out, attn_out], dim=1)))\n",
    "            fused_attn = gate_attn * lstm_out + (1 - gate_attn) * attn_out\n",
    "            fused = fused_attn + basis_out\n",
    "        else:  # concat\n",
    "            fused = torch.cat([lstm_out, attn_out, basis_out], dim=1)\n",
    "        realization_emb = self.realization_embedding(realization_ids)\n",
    "        combined = torch.cat([fused, realization_emb], dim=1)\n",
    "        x = self.fc1(combined)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735bc59c",
   "metadata": {
    "papermill": {
     "duration": 0.008443,
     "end_time": "2025-06-01T15:24:44.382893",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.374450",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Supplementary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a93b99b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:44.400995Z",
     "iopub.status.busy": "2025-06-01T15:24:44.400773Z",
     "iopub.status.idle": "2025-06-01T15:24:44.406958Z",
     "shell.execute_reply": "2025-06-01T15:24:44.406395Z"
    },
    "papermill": {
     "duration": 0.016733,
     "end_time": "2025-06-01T15:24:44.408252",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.391519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_nll_score(solution, submission, row_id_column_name='geology_id'):\n",
    "    solution_copy = solution.copy()\n",
    "    submission_copy = submission.copy()\n",
    "\n",
    "    del solution_copy[row_id_column_name]\n",
    "    del submission_copy[row_id_column_name]\n",
    "\n",
    "    NEGATIVE_PART = -299\n",
    "    LARGEST_CHUNK = 600\n",
    "    SMALLEST_CHUNK = 350\n",
    "    TOTAL_REALIZATIONS = 10\n",
    "    INFLATION_SIGMA = 600\n",
    "    \n",
    "    sigma_2 = np.ones((LARGEST_CHUNK+NEGATIVE_PART-1))\n",
    "    from_ranges = [1, 61, 245]\n",
    "    to_ranges_excl = [61, 245, 301]\n",
    "    log_slopes = [1.0406028049510443, 0.0, 7.835345062351012]\n",
    "    log_offsets = [-6.430669850650689, -2.1617411566043896, -45.24876794412965]\n",
    "\n",
    "    for growth_mode in range(len(from_ranges)):\n",
    "        for i in range(from_ranges[growth_mode], to_ranges_excl[growth_mode]):\n",
    "            sigma_2[i-1] = np.exp(np.log(i)*log_slopes[growth_mode]+log_offsets[growth_mode])\n",
    "\n",
    "    sigma_2 *= INFLATION_SIGMA\n",
    "  \n",
    "    cov_matrix_inv_diag = 1. / sigma_2\n",
    "    \n",
    "    num_rows = solution_copy.shape[0]\n",
    "    num_columns = LARGEST_CHUNK + NEGATIVE_PART - 1\n",
    "    \n",
    "    p = 1./TOTAL_REALIZATIONS\n",
    "    log_p = np.log(p)\n",
    "    \n",
    "    solution_arr = np.zeros((num_rows, TOTAL_REALIZATIONS, num_columns))\n",
    "    submission_arr = np.zeros((num_rows, TOTAL_REALIZATIONS, num_columns))\n",
    "    \n",
    "    for k in range(TOTAL_REALIZATIONS):\n",
    "        for i in range(num_columns):\n",
    "            column_name = f\"r_{k}_pos_{i+1}\"\n",
    "            solution_arr[:, k, i] = solution_copy[column_name].values\n",
    "            submission_arr[:, k, i] = submission_copy[column_name].values\n",
    "\n",
    "    misfit = solution_arr - submission_arr\n",
    "    inner_product_matrix = np.sum(cov_matrix_inv_diag * misfit * misfit, axis=2)\n",
    "    \n",
    "    nll = -logsumexp(log_p - inner_product_matrix, axis=1)\n",
    "    \n",
    "    return nll.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dc237fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:44.426454Z",
     "iopub.status.busy": "2025-06-01T15:24:44.426239Z",
     "iopub.status.idle": "2025-06-01T15:24:44.430800Z",
     "shell.execute_reply": "2025-06-01T15:24:44.430231Z"
    },
    "papermill": {
     "duration": 0.014977,
     "end_time": "2025-06-01T15:24:44.431904",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.416927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_with_nll_loss(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for data, target, rid in train_loader:\n",
    "        data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        rid = rid.to(device)\n",
    "        output = model(data, rid)\n",
    "        \n",
    "        target_mean = target.mean(dim=0)\n",
    "        target_std = target.std(dim=0) + 1e-6\n",
    "\n",
    "        normalized_output = (output - target_mean) / target_std\n",
    "        normalized_target = (target - target_mean) / target_std\n",
    "\n",
    "        loss = F.mse_loss(normalized_output, normalized_target)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    return np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d45da15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:44.450138Z",
     "iopub.status.busy": "2025-06-01T15:24:44.449917Z",
     "iopub.status.idle": "2025-06-01T15:24:44.454916Z",
     "shell.execute_reply": "2025-06-01T15:24:44.454313Z"
    },
    "papermill": {
     "duration": 0.015641,
     "end_time": "2025-06-01T15:24:44.456121",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.440480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target, rid in val_loader:\n",
    "            data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.float32)\n",
    "            rid = rid.to(device)\n",
    "            output = model(data, rid)\n",
    "            \n",
    "            target_mean = target.mean(dim=0)\n",
    "            target_std = target.std(dim=0) + 1e-6\n",
    "\n",
    "            normalized_output = (output - target_mean) / target_std\n",
    "            normalized_target = (target - target_mean) / target_std\n",
    "            \n",
    "            loss = F.mse_loss(normalized_output, normalized_target)\n",
    "            \n",
    "            val_losses.append(loss.item())\n",
    "            val_preds.append(output.cpu().numpy())\n",
    "            val_targets.append(target.cpu().numpy())\n",
    "    \n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_targets = np.concatenate(val_targets)\n",
    "    \n",
    "    return np.mean(val_losses), val_preds, val_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "773fc3cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:44.474003Z",
     "iopub.status.busy": "2025-06-01T15:24:44.473794Z",
     "iopub.status.idle": "2025-06-01T15:24:44.482651Z",
     "shell.execute_reply": "2025-06-01T15:24:44.481866Z"
    },
    "papermill": {
     "duration": 0.019184,
     "end_time": "2025-06-01T15:24:44.483899",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.464715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "def init_wandb(project_name=\"geology-forecast-challenge\", config=None):\n",
    "    try:\n",
    "        user_secrets = UserSecretsClient()\n",
    "        \n",
    "        wandb_api_key = user_secrets.get_secret(\"wandb\")\n",
    "        os.environ['WANDB_API_KEY'] = wandb_api_key\n",
    "        \n",
    "        wandb.login(key=wandb_api_key)\n",
    "        \n",
    "        run = wandb.init(\n",
    "            project=project_name,\n",
    "            config=config,\n",
    "            tags=[\"LSTM\", \"Geology Forecast Challenge\"],\n",
    "        )\n",
    "        \n",
    "        print(\"W&B successfully initialized\")\n",
    "        return run\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing W&B: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959cc02d",
   "metadata": {
    "papermill": {
     "duration": 0.008498,
     "end_time": "2025-06-01T15:24:44.501177",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.492679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Function to improve data variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b68c471a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:44.518947Z",
     "iopub.status.busy": "2025-06-01T15:24:44.518748Z",
     "iopub.status.idle": "2025-06-01T15:24:44.522039Z",
     "shell.execute_reply": "2025-06-01T15:24:44.521484Z"
    },
    "papermill": {
     "duration": 0.013327,
     "end_time": "2025-06-01T15:24:44.523066",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.509739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mixup(data, targets, alpha=0.4):\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    index = np.random.permutation(len(data))\n",
    "    return lam * data + (1 - lam) * data[index], lam * targets + (1 - lam) * targets[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b101a018",
   "metadata": {
    "papermill": {
     "duration": 0.008391,
     "end_time": "2025-06-01T15:24:44.540227",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.531836",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88ca9130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:44.558251Z",
     "iopub.status.busy": "2025-06-01T15:24:44.558005Z",
     "iopub.status.idle": "2025-06-01T15:24:44.569444Z",
     "shell.execute_reply": "2025-06-01T15:24:44.568874Z"
    },
    "papermill": {
     "duration": 0.021756,
     "end_time": "2025-06-01T15:24:44.570596",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.548840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_predict(\n",
    "    fold_idx, \n",
    "    train_index, \n",
    "    val_index, \n",
    "    X_num, \n",
    "    y,\n",
    "    X_num_test,\n",
    "    config,\n",
    "    train_sub=None\n",
    "):\n",
    "    fold_config = config.copy()\n",
    "    fold_config.update({\"fold\": fold_idx})\n",
    "    \n",
    "    run = init_wandb(config=fold_config)\n",
    "    \n",
    "    # Train data (EXPANDED across 10 realizations)\n",
    "    X_num_train = X_num[train_index].reshape((-1,300))\n",
    "    y_train = y[train_index].reshape((-1,300))\n",
    "    realization_ids_train = np.tile(np.arange(NUM_REALIZATIONS), len(train_index))\n",
    "    \n",
    "    # Validation data (USE ONLY REALIZATION 0)\n",
    "    X_num_val = X_num[val_index][:, :300]\n",
    "    y_val = y[val_index][:, :300]\n",
    "    realization_ids_val = np.zeros(len(X_num_val), dtype=int)\n",
    "\n",
    "    X_num_train, y_train = mixup(X_num_train, y_train, 0.4)\n",
    "    \n",
    "    train_dataset = GeologyDataset(X_num_train, y_train, realization_ids_train)\n",
    "    val_dataset = GeologyDataset(X_num_val, y_val, realization_ids_val)\n",
    "    test_dataset = GeologyDataset(X_num_test, is_test=True)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True,\n",
    "        pin_memory=True, \n",
    "        num_workers=2  \n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    model = ParallelLSTMWithAttention(\n",
    "        input_size=1,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        output_size=300,\n",
    "        dropout=config['dropout'],\n",
    "        num_realizations=NUM_REALIZATIONS,\n",
    "        realization_emb_dim=16,\n",
    "        use_multihead=True,\n",
    "        num_heads=2,\n",
    "        fusion_method='concat'  # 'concat' or 'add', 'gated'\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        eps=1e-8  # Increased stability\n",
    "    )\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=5,       \n",
    "        T_mult=2,   \n",
    "        eta_min=1e-6 \n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    val_predictions = np.zeros((len(val_index), len(REALIZATIONS)))\n",
    "    test_predictions = np.zeros((len(X_num_test), len(REALIZATIONS)))\n",
    "\n",
    "    print(f\"Training fold {fold_idx + 1}...\")\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_loss = train_model_with_nll_loss(model, train_loader, optimizer, device)\n",
    "        \n",
    "        val_loss, val_preds, val_targets = validate_model(model, val_loader, device)\n",
    "        \n",
    "        val_predictions = val_preds\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        if run:\n",
    "            run.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            model_path = f\"model_fold_{fold_idx}.pt\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            if run:\n",
    "                run.save(model_path)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"model_fold_{fold_idx}.pt\"))\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            if isinstance(data, list) or isinstance(data, tuple):\n",
    "                data = data[0]\n",
    "            data = data.to(device, dtype=torch.float32)\n",
    "    \n",
    "            outputs = []\n",
    "            for r_id in range(NUM_REALIZATIONS):\n",
    "                realization_ids = torch.full((data.size(0),), r_id, dtype=torch.long, device=device)\n",
    "                preds = model(data, realization_ids)  # [B, 300]\n",
    "                outputs.append(preds.cpu().numpy())   # Append [B, 300]\n",
    "    \n",
    "            # Stack into shape: [B, 10, 300] ‚Üí then reshape to [B, 3000]\n",
    "            outputs = np.stack(outputs, axis=1).reshape(data.size(0), 300 * NUM_REALIZATIONS)\n",
    "            test_preds.append(outputs)\n",
    "    \n",
    "    test_predictions = np.concatenate(test_preds)\n",
    "\n",
    "    train_sub.loc[val_index, REALIZATIONS[:300]] = val_predictions\n",
    "    \n",
    "    if run:\n",
    "        run.finish()\n",
    "    \n",
    "    return test_predictions, train_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcde8df",
   "metadata": {
    "papermill": {
     "duration": 0.008533,
     "end_time": "2025-06-01T15:24:44.587795",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.579262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363ca30",
   "metadata": {
    "papermill": {
     "duration": 0.008441,
     "end_time": "2025-06-01T15:24:44.605304",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.596863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ====================\n",
    "# üöß Optional Training Block\n",
    "# ====================\n",
    "# Uncomment the block below to retrain the model from scratch.\n",
    "# Training takes approx. 8 hours on a P100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d38f65e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:44.623373Z",
     "iopub.status.busy": "2025-06-01T15:24:44.623106Z",
     "iopub.status.idle": "2025-06-01T15:24:44.626135Z",
     "shell.execute_reply": "2025-06-01T15:24:44.625545Z"
    },
    "papermill": {
     "duration": 0.013378,
     "end_time": "2025-06-01T15:24:44.627266",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.613888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SEED = 2025\n",
    "# config = {\n",
    "#     'model_type': 'LSTM',\n",
    "#     'hidden_size': 1024,\n",
    "#     'num_layers': 3,\n",
    "#     'dropout': 0.2,\n",
    "#     'learning_rate': 5e-4,\n",
    "#     'weight_decay': 1e-5,\n",
    "#     'batch_size': 256,\n",
    "#     'epochs': 30,\n",
    "#     'seed': SEED,\n",
    "# }\n",
    "\n",
    "# train_sub = train_df[['geology_id'] + train_REALIZATIONS].copy()\n",
    "# solution = train_df[['geology_id'] + train_REALIZATIONS].copy()\n",
    "\n",
    "# folds = 5\n",
    "# kf = KFold(n_splits=folds, random_state=SEED, shuffle=True)\n",
    "\n",
    "# X_num, y = train_df[train_FEATURES].values, train_df[train_REALIZATIONS].values # NOTE: We take one realization only train_df.iloc[:,301:601].values\n",
    "# X_num_test = test_df[FEATURES].values\n",
    "\n",
    "# test_preds_all_folds = np.zeros((folds, len(test_df), len(REALIZATIONS)))\n",
    "# val_scores = []\n",
    "\n",
    "# for fold_idx, (train_index, val_index) in enumerate(kf.split(X_num)):\n",
    "#     test_preds, train_sub = train_and_predict(\n",
    "#         fold_idx, \n",
    "#         train_index, \n",
    "#         val_index, \n",
    "#         X_num, \n",
    "#         y,\n",
    "#         X_num_test,\n",
    "#         config,\n",
    "#         train_sub\n",
    "#     )\n",
    "#     test_preds_all_folds[fold_idx] = test_preds\n",
    "    \n",
    "#     fold_val_preds = train_sub.loc[val_index, ['geology_id'] + train_REALIZATIONS]\n",
    "#     fold_val_solution = solution.loc[val_index]\n",
    "    \n",
    "#     fold_score = compute_nll_score(fold_val_solution, fold_val_preds)\n",
    "#     val_scores.append(fold_score)\n",
    "    \n",
    "#     print(f\"Fold {fold_idx+1} validation NLL score: {fold_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af579713",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:44.645396Z",
     "iopub.status.busy": "2025-06-01T15:24:44.645164Z",
     "iopub.status.idle": "2025-06-01T15:24:44.647777Z",
     "shell.execute_reply": "2025-06-01T15:24:44.647200Z"
    },
    "papermill": {
     "duration": 0.012852,
     "end_time": "2025-06-01T15:24:44.648919",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.636067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# avg_val_score = np.mean(val_scores)\n",
    "# print(f\"Average validation NLL score: {avg_val_score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae6e1a",
   "metadata": {
    "papermill": {
     "duration": 0.008447,
     "end_time": "2025-06-01T15:24:44.666084",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.657637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33cbe0f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:44.685413Z",
     "iopub.status.busy": "2025-06-01T15:24:44.685173Z",
     "iopub.status.idle": "2025-06-01T15:24:44.687936Z",
     "shell.execute_reply": "2025-06-01T15:24:44.687348Z"
    },
    "papermill": {
     "duration": 0.013317,
     "end_time": "2025-06-01T15:24:44.689389",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.676072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # üíæ Save Submission\n",
    "# test_preds_avg = np.mean(test_preds_all_folds, axis=0)\n",
    "# submission = submission_template.copy()\n",
    "# submission[REALIZATIONS] = test_preds_avg\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# print(\"‚úÖ Submission saved as submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47aeb66",
   "metadata": {
    "papermill": {
     "duration": 0.008361,
     "end_time": "2025-06-01T15:24:44.706407",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.698046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Code for prediction without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa52fb76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:24:44.724208Z",
     "iopub.status.busy": "2025-06-01T15:24:44.723984Z",
     "iopub.status.idle": "2025-06-01T15:26:03.754440Z",
     "shell.execute_reply": "2025-06-01T15:26:03.753420Z"
    },
    "papermill": {
     "duration": 79.049079,
     "end_time": "2025-06-01T15:26:03.763991",
     "exception": false,
     "start_time": "2025-06-01T15:24:44.714912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-3c3568ca0bc4>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f\"{weights_path}/model_fold_{fold}.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Submission saved as submission.csv\n"
     ]
    }
   ],
   "source": [
    "SEED = 2025\n",
    "config = {\n",
    "    'model_type': 'LSTM',\n",
    "    'hidden_size': 1024,\n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.2,\n",
    "    'learning_rate': 5e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'batch_size': 256,\n",
    "    'epochs': 30,\n",
    "    'seed': SEED,\n",
    "}\n",
    "FOLDS = 5\n",
    "\n",
    "X_num_test = test_df[FEATURES].values\n",
    "test_dataset = GeologyDataset(X_num_test, is_test=True)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "weights_path = \"/kaggle/input/geologyforecast-model-1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_preds_all_folds = np.zeros((FOLDS, len(test_df), len(REALIZATIONS)))\n",
    "for fold in range(FOLDS):\n",
    "    model = ParallelLSTMWithAttention(\n",
    "        input_size=1,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        output_size=300,\n",
    "        dropout=config['dropout'],\n",
    "        num_realizations=NUM_REALIZATIONS,\n",
    "        realization_emb_dim=16,\n",
    "        use_multihead=True,\n",
    "        num_heads=2,\n",
    "        fusion_method='concat'  # 'concat' or 'add', 'gated'\n",
    "    ).to(device)\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"{weights_path}/model_fold_{fold}.pt\", map_location=device)\n",
    "    )\n",
    "    model.eval()\n",
    "    test_predictions = np.zeros((len(X_num_test), len(REALIZATIONS)))\n",
    "    test_preds = []    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            if isinstance(data, list) or isinstance(data, tuple):\n",
    "                data = data[0]\n",
    "            data = data.to(device, dtype=torch.float32)\n",
    "    \n",
    "            outputs = []\n",
    "            for r_id in range(NUM_REALIZATIONS):\n",
    "                realization_ids = torch.full((data.size(0),), r_id, dtype=torch.long, device=device)\n",
    "                preds = model(data, realization_ids)  # [B, 300]\n",
    "                outputs.append(preds.cpu().numpy())   # Append [B, 300]\n",
    "    \n",
    "            # Stack into shape: [B, 10, 300] ‚Üí then reshape to [B, 3000]\n",
    "            outputs = np.stack(outputs, axis=1).reshape(data.size(0), 300 * NUM_REALIZATIONS)\n",
    "            test_preds.append(outputs)\n",
    "    \n",
    "    test_predictions = np.concatenate(test_preds)\n",
    "    test_preds_all_folds[fold] = test_predictions\n",
    "\n",
    "test_preds_avg = np.mean(test_preds_all_folds, axis=0)\n",
    "submission = submission_template.copy()\n",
    "submission[REALIZATIONS] = test_preds_avg\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"‚úÖ Submission saved as submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43fbe7ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T15:26:03.782908Z",
     "iopub.status.busy": "2025-06-01T15:26:03.782603Z",
     "iopub.status.idle": "2025-06-01T15:26:03.821045Z",
     "shell.execute_reply": "2025-06-01T15:26:03.820296Z"
    },
    "papermill": {
     "duration": 0.049246,
     "end_time": "2025-06-01T15:26:03.822224",
     "exception": false,
     "start_time": "2025-06-01T15:26:03.772978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geology_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>r_9_pos_291</th>\n",
       "      <th>r_9_pos_292</th>\n",
       "      <th>r_9_pos_293</th>\n",
       "      <th>r_9_pos_294</th>\n",
       "      <th>r_9_pos_295</th>\n",
       "      <th>r_9_pos_296</th>\n",
       "      <th>r_9_pos_297</th>\n",
       "      <th>r_9_pos_298</th>\n",
       "      <th>r_9_pos_299</th>\n",
       "      <th>r_9_pos_300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>g_4a52df537a</td>\n",
       "      <td>-0.007445</td>\n",
       "      <td>-0.016354</td>\n",
       "      <td>-0.025799</td>\n",
       "      <td>-0.034601</td>\n",
       "      <td>-0.043966</td>\n",
       "      <td>-0.053893</td>\n",
       "      <td>-0.063475</td>\n",
       "      <td>-0.073740</td>\n",
       "      <td>-0.080297</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.413546</td>\n",
       "      <td>-2.425948</td>\n",
       "      <td>-2.434349</td>\n",
       "      <td>-2.445190</td>\n",
       "      <td>-2.457541</td>\n",
       "      <td>-2.463572</td>\n",
       "      <td>-2.479977</td>\n",
       "      <td>-2.482481</td>\n",
       "      <td>-2.496219</td>\n",
       "      <td>-2.497137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>g_1e4b5a1509</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>0.025368</td>\n",
       "      <td>0.042212</td>\n",
       "      <td>0.054267</td>\n",
       "      <td>0.066772</td>\n",
       "      <td>0.080740</td>\n",
       "      <td>0.094610</td>\n",
       "      <td>0.108543</td>\n",
       "      <td>0.122402</td>\n",
       "      <td>...</td>\n",
       "      <td>4.056398</td>\n",
       "      <td>4.065425</td>\n",
       "      <td>4.081152</td>\n",
       "      <td>4.093845</td>\n",
       "      <td>4.107855</td>\n",
       "      <td>4.125825</td>\n",
       "      <td>4.136027</td>\n",
       "      <td>4.150600</td>\n",
       "      <td>4.162697</td>\n",
       "      <td>4.185105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>g_5919c0bea3</td>\n",
       "      <td>0.024914</td>\n",
       "      <td>0.054046</td>\n",
       "      <td>0.089179</td>\n",
       "      <td>0.114869</td>\n",
       "      <td>0.142523</td>\n",
       "      <td>0.172546</td>\n",
       "      <td>0.200963</td>\n",
       "      <td>0.231460</td>\n",
       "      <td>0.261015</td>\n",
       "      <td>...</td>\n",
       "      <td>8.098316</td>\n",
       "      <td>8.121840</td>\n",
       "      <td>8.148428</td>\n",
       "      <td>8.174735</td>\n",
       "      <td>8.196062</td>\n",
       "      <td>8.232018</td>\n",
       "      <td>8.258633</td>\n",
       "      <td>8.288578</td>\n",
       "      <td>8.313424</td>\n",
       "      <td>8.344357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>g_9a665aae6d</td>\n",
       "      <td>0.037737</td>\n",
       "      <td>0.080991</td>\n",
       "      <td>0.132355</td>\n",
       "      <td>0.170564</td>\n",
       "      <td>0.211298</td>\n",
       "      <td>0.255330</td>\n",
       "      <td>0.297738</td>\n",
       "      <td>0.342559</td>\n",
       "      <td>0.385908</td>\n",
       "      <td>...</td>\n",
       "      <td>11.010870</td>\n",
       "      <td>11.036271</td>\n",
       "      <td>11.068656</td>\n",
       "      <td>11.106090</td>\n",
       "      <td>11.133821</td>\n",
       "      <td>11.179789</td>\n",
       "      <td>11.221371</td>\n",
       "      <td>11.256356</td>\n",
       "      <td>11.283451</td>\n",
       "      <td>11.329015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>g_ba4abe1b9e</td>\n",
       "      <td>0.019050</td>\n",
       "      <td>0.040938</td>\n",
       "      <td>0.067726</td>\n",
       "      <td>0.087351</td>\n",
       "      <td>0.107455</td>\n",
       "      <td>0.129143</td>\n",
       "      <td>0.152126</td>\n",
       "      <td>0.174153</td>\n",
       "      <td>0.197243</td>\n",
       "      <td>...</td>\n",
       "      <td>5.733640</td>\n",
       "      <td>5.745847</td>\n",
       "      <td>5.766169</td>\n",
       "      <td>5.786160</td>\n",
       "      <td>5.801396</td>\n",
       "      <td>5.828416</td>\n",
       "      <td>5.843224</td>\n",
       "      <td>5.862846</td>\n",
       "      <td>5.878823</td>\n",
       "      <td>5.903850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     geology_id         1         2         3         4         5         6  \\\n",
       "0  g_4a52df537a -0.007445 -0.016354 -0.025799 -0.034601 -0.043966 -0.053893   \n",
       "1  g_1e4b5a1509  0.011655  0.025368  0.042212  0.054267  0.066772  0.080740   \n",
       "2  g_5919c0bea3  0.024914  0.054046  0.089179  0.114869  0.142523  0.172546   \n",
       "3  g_9a665aae6d  0.037737  0.080991  0.132355  0.170564  0.211298  0.255330   \n",
       "4  g_ba4abe1b9e  0.019050  0.040938  0.067726  0.087351  0.107455  0.129143   \n",
       "\n",
       "          7         8         9  ...  r_9_pos_291  r_9_pos_292  r_9_pos_293  \\\n",
       "0 -0.063475 -0.073740 -0.080297  ...    -2.413546    -2.425948    -2.434349   \n",
       "1  0.094610  0.108543  0.122402  ...     4.056398     4.065425     4.081152   \n",
       "2  0.200963  0.231460  0.261015  ...     8.098316     8.121840     8.148428   \n",
       "3  0.297738  0.342559  0.385908  ...    11.010870    11.036271    11.068656   \n",
       "4  0.152126  0.174153  0.197243  ...     5.733640     5.745847     5.766169   \n",
       "\n",
       "   r_9_pos_294  r_9_pos_295  r_9_pos_296  r_9_pos_297  r_9_pos_298  \\\n",
       "0    -2.445190    -2.457541    -2.463572    -2.479977    -2.482481   \n",
       "1     4.093845     4.107855     4.125825     4.136027     4.150600   \n",
       "2     8.174735     8.196062     8.232018     8.258633     8.288578   \n",
       "3    11.106090    11.133821    11.179789    11.221371    11.256356   \n",
       "4     5.786160     5.801396     5.828416     5.843224     5.862846   \n",
       "\n",
       "   r_9_pos_299  r_9_pos_300  \n",
       "0    -2.496219    -2.497137  \n",
       "1     4.162697     4.185105  \n",
       "2     8.313424     8.344357  \n",
       "3    11.283451    11.329015  \n",
       "4     5.878823     5.903850  \n",
       "\n",
       "[5 rows x 3001 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11372669,
     "isSourceIdPinned": false,
     "sourceId": 95697,
     "sourceType": "competition"
    },
    {
     "datasetId": 7557838,
     "sourceId": 12013599,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 174.836153,
   "end_time": "2025-06-01T15:26:07.280437",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-01T15:23:12.444284",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
