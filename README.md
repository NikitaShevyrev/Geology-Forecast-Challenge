# Geology Forecast Challenge â€” Solution by Nikita Shevyrev

This repository contains the complete solution for the [Kaggle Geology Forecast Challenge](https://www.kaggle.com/competitions/geology-forecast-challenge).

The solution combines:
- ğŸ“ˆ A powerful LSTM + Multi-head Attention architecture
- ğŸ§  A parallel **learnable attention projection** block
- ğŸ§ª Synthetic training data generation
- ğŸ” 5-fold ensembling with 10 realization predictions per sample

---

## ğŸ“‚ Repository Structure
GEOLOGY-FORECAST-CHALLENGE/
â”œâ”€â”€ augmentations/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ mixup.py                  â† Mixup data augmentation
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_generator.py         â† Data generation function
â”‚   â””â”€â”€ geology_dataset.py        â† Custom PyTorch Dataset
â”‚
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ evaluation.py             â† Metric computation (e.g. NLL)
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ lstm_with_attention.py    â† Final model architecture
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ loop.py                   â† Training and validation functions
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ logging.py                â† Weights & Biases setup
â”‚
â”œâ”€â”€ geologyforecast.ipynb         â† The final competition notebook
â”œâ”€â”€ inference.py                  â† Script to generate submission.csv
â”œâ”€â”€ LICENSE                       â† License file
â”œâ”€â”€ README.md                     â† README file
â”œâ”€â”€ requirements.txt              â† Python dependencies
â””â”€â”€ train.py                      â† Training pipeline with data preparation

---

## âš™ï¸ Environment Setup

You can install required dependencies with:
```bash
pip install -r requirements.txt
```

Tested on: Python 3.10, PyTorch 1.13+

---

## ğŸ‹ï¸â€â™‚ï¸ Training and Synthetic Data Generation

Steps:

1. Ensure your `input/` folder contains:
```
train.csv, sample_submission.csv, test.csv
```

2. Run the training script (set `RUN_DATA_GENERATION = False` if youâ€™ve already generated data):
```bash
python train.py
```

3. Folded weights will be saved to:
```
weights/model_fold_0.pt ... model_fold_4.pt
```

Expected Folder Layout to run `train.py`:
GEOLOGY-FORECAST-CHALLENGE/
â”œâ”€â”€ train.py
â”œâ”€â”€ weights/
â”‚   â”œâ”€â”€ model_fold_0.pt
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ input/
â”‚   â””â”€â”€ geology-forecast-challenge-open/
â”‚       â””â”€â”€ data/
â”‚           â”œâ”€â”€ train.csv
â”‚           â”œâ”€â”€ test.csv
â”‚           â”œâ”€â”€ sample_submission.csv
â”‚           â””â”€â”€ train_raw/

---

## ğŸ“ˆ Inference

Steps:

1. Place the official test data in:
```
input/geology-forecast-challenge-open/data/test.csv
input/geology-forecast-challenge-open/data/sample_submission.csv
```

2. Place the 5 trained model weights in:
weights/
â”œâ”€â”€ model_fold_0.pt
â”œâ”€â”€ ...
â””â”€â”€ model_fold_4.pt

3. Run Inference:
```bash
python inference.py
```

- This script performs ensemble inference across 5 folds Ã— 10 realizations per sample.
- The resulting `submission.csv` is saved in the root folder and can be submitted to Kaggle.
- **Note:** This script does not require training. It loads only the provided weights and test files.

---

## ğŸ“„ Reproducibility & Competition Compliance
This repository contains:

âœ… Full code for data preparation, model, training, and inference

âœ… Scripts to train from scratch or reproduce predictions using saved weights

âœ… A Kaggle notebook for competition submission

---

## ğŸ” Note on Reproducibility & Model Weights
Due to small differences in numerical behavior across hardware (e.g., Kaggleâ€™s P100 vs my local GPU) and library versions, I was **unable to reproduce identical weights across environments**. This can affect the final score even with the same code and seed.

For that reason:
- ğŸ”’ I trained the final model **locally** using my own GPU (RTX 3080 Ti)
- ğŸ“¦ The resulting weights are saved in a **private Kaggle dataset**, which is loaded in the final notebook to ensure accurate reproduction of `submission.csv`
- âœ… The notebook contains **full training code** (commented) in accordance with competition requirements

If anyone needs access to the weights for review or reproduction, feel free to:
- Open an issue here, or
- Contact me via Kaggle

---

## ğŸ™ Acknowledgements & Inspiration
Several Kaggle community members published excellent notebooks that inspired key aspects of this solution:

1. **Eva Koroleva**
- ğŸ§  Idea: Using **LSTM** as a base model
- ğŸ““ Notebook: [LSTM KFold](https://www.kaggle.com/code/qmarva/lstm-kfold)
- ğŸ‘¤ Profile: [@qmarva](https://www.kaggle.com/qmarva)

2. **OlSch**
- ğŸ§  Idea: Using **skip connections (residual blocks)** inside neural network stacks
- ğŸ““ Notebook: [Keras Geology Forecast Competition 58.4](https://www.kaggle.com/code/olasdsd/keras-geology-forecast-competition-58-4)
- ğŸ‘¤ Profile: [@olasdsd](https://www.kaggle.com/olasdsd)
- ğŸ’¡ This structure (shown below) led me to introduce **residual LSTM stacks**, which significantly improved my model performance.
```python
# OlSch's solution (Keras) with a skip connection block
def initModel(num_features): 
    num_input = Input(shape=(num_features,), dtype="float32", name="num_input")
    x = Dense(UNITS, activation='linear')(num_input)
    for i in range(DEPT):
        skip = x
        x = Dense(UNITS, activation=ACTIVATION)(x)
        x = Dropout(DROPOUT)(x)
        x = x + skip
    y_pred = Dense(3000, activation='linear')(x)
        
    model = tf.keras.Model(
        inputs = [num_input],
        outputs = [y_pred]
    )
            
    opt = tf.keras.optimizers.Adam(learning_rate = 1e-4)
    model.compile(loss='mse', optimizer = opt)

    return model
```

Thanks to the Kaggle community for openly sharing ideas â€” it allowed me to iterate faster and build a stronger final solution.

---

## ğŸ“¬ Contact
Feel free to open an issue or message me on Kaggle for questions or clarification.
